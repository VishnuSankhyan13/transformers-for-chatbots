The transformer architecture introduced in the 'Attention is All You Need' paper consists of a stack of N = 6 identical layers for both the encoder and decoder. Each layer in the encoder has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. Similarly, each layer in the decoder has two sub-layers and an additional third sub-layer that performs multi-head attention over the output of the encoder stack. Residual connections and layer normalization are applied after each sub-layer.